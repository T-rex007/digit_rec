{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"digit_rec_neural_net.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"MP-hmkO9dvg1","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","#from tflearn.layers.core import input_data\n","#from tflearn.data_augmentation import ImageAugmentation"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OoDBMHFMdvhA","colab_type":"code","colab":{}},"cell_type":"code","source":["### neural layer # not used\n","def neuron_layer(x, n_neurons, name, activation = None):\n","    with tf.name_scope(name):\n","        n_inputs = int(x.get_shape()[1])\n","        stddev = 2/ np.sqrt(n_inputs)\n","        init = tf.truncated_normal((n_inputs, n_neurons), stddev = stddev)\n","        w = tf.Variable(init, name = \"weights\")\n","        b = tf.Variable(tf.zeros([n_neurons], tf.float32))\n","        z = tf.add(tf.matmul(x,w),b)\n","        if activation == 'relu':\n","            return tf.nn.relu(z)\n","        elif activation == 'softmax': \n","            return tf.nn.softmax(z)\n","        else:\n","            return z\n","        \n","### Loading in and preprocessing the data\n","def load_prep(path, testortrain = 'train' ):\n","    data = pd.read_csv(path)\n","    \n","    from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n","    if testortrain == 'train':\n","        features = np.array(data.drop('label', axis = 1))\n","        labels = data.label.values\n","        features = StandardScaler().fit_transform(np.float32(features))\n","        return features, labels\n","    elif testortrain == 'test':\n","        features = np.array(data)\n","        features = StandardScaler().fit_transform(np.float32(features))\n","        return features\n","    else:\n","        print(\"-Test or Train ?\")\n","        return None\n","\n","def leaky_relu(z, name = None):\n","    return tf.maximum(0.01* z, z, name = name)\n","\n","def generator(features, labels, batch_size):\n","    for i in range(0, len(features), batch_size):\n","        yield (features[i:i+batch_size], labels[i:i+batch_size])\n","        \n","def max_norm_regularizer(threshold = 1.0, axes = 1, name ='max_norm', collection = 'max_norm'):\n","    def max_norm(weights):\n","        clipped_weights = tf.clip_by_norm(weights, clip_norm = threshold, axes = axes)\n","        clip_weights = tf.assign(weights, clipped_weights, name =name)\n","        tf.add_to_collection(collection, clip_weights)\n","        return None\n","    return max_norm"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XGbyhnlpdvhH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1033},"outputId":"259fc1d0-1da7-4a9f-c9c5-8e450d2af5cd","executionInfo":{"status":"error","timestamp":1547857648824,"user_tz":240,"elapsed":1295,"user":{"displayName":"shaqeal Cadogan","photoUrl":"https://lh5.googleusercontent.com/-ElhIDm0SpK8/AAAAAAAAAAI/AAAAAAAAC2g/uvEzPBRYlt8/s64/photo.jpg","userId":"18054494821555600959"}}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","features, labels = load_prep(path ='train.csv', testortrain = 'train')\n","xtrain, xtest, ytrain, ytest = train_test_split(features, labels, test_size = .25,\n","                                                shuffle = True, stratify = labels,\n","                                                random_state = 42)\n","\n","n_inputs = features.shape[1]\n","n_hidden1 = 300\n","n_hidden2 = 100\n","n_hidden3 = 50\n","n_outputs = 10"],"execution_count":3,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-42d68b8f3d0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestortrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m xtrain, xtest, ytrain, ytest = train_test_split(features, labels, test_size = .25,\n\u001b[1;32m      5\u001b[0m                                                 \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-b072cb6f1005>\u001b[0m in \u001b[0;36mload_prep\u001b[0;34m(path, testortrain)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m### Loading in and preprocessing the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestortrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: File b'train.csv' does not exist"]}]},{"metadata":{"id":"dGDGntCEdvhQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":231},"outputId":"b05bf0de-be54-4b4c-e3b1-de2ff4e9fbd3","executionInfo":{"status":"error","timestamp":1547857658415,"user_tz":240,"elapsed":9875,"user":{"displayName":"shaqeal Cadogan","photoUrl":"https://lh5.googleusercontent.com/-ElhIDm0SpK8/AAAAAAAAAAI/AAAAAAAAC2g/uvEzPBRYlt8/s64/photo.jpg","userId":"18054494821555600959"}}},"cell_type":"code","source":["from tensorflow.contrib.layers import batch_norm, dropout\n","\n","x = tf.placeholder(tf.float32, shape = [ None, n_inputs], name = \"x\")\n","yt = tf.placeholder(tf.int64, shape = (None), name = \"ytrue\")\n","\n","is_training = tf.placeholder(tf.bool, shape = (), name ='is_training')\n","keep_prob = 0.8\n","x_drop = dropout(x, keep_prob, is_training = is_training )\n","    \n","from tensorflow.contrib.layers import fully_connected\n","with tf.name_scope('dnn'):\n","    np.random.seed()\n","    he_init = tf.contrib.layers.variance_scaling_initializer()\n","    \n","    hidden1 = fully_connected(x_drop, n_hidden1, weights_initializer = he_init, scope = 'h1',\n","                              activation_fn  = tf.nn.elu)\n","    hidden1_drop = dropout(hidden1, keep_prob, is_training = is_training)\n","    \n","    hidden2 = fully_connected(hidden1_drop, n_hidden2, \n","                              weights_initializer = he_init, scope = 'h2',\n","                              activation_fn = tf.nn.elu)\n","    hidden2_drop = dropout(hidden2, keep_prob, is_training = is_training)\n","    \n","    hidden3 = fully_connected(hidden2, n_hidden3, weights_initializer = he_init, scope = 'h3',\n","                             activation_fn = tf.nn.elu)\n","    hidden3_drop = dropout(hidden3, keep_prob, is_training = is_training)\n","    \n","    logits = fully_connected(hidden3_drop, n_outputs,weights_initializer = he_init,\n","                             scope = 'outputs', activation_fn = None)\n","\n","with tf.name_scope('loss'):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = yt, logits = logits)\n","    lossb = tf.reduce_mean(xentropy, name = 'avg_xentropy')\n","    reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","    loss = tf.add_n([lossb]+ reg_loss, name = 'loss')\n","    \n","learning_rate = 0.001\n","with tf.name_scope('train'):\n","    optimizer = tf.train.AdamOptimizer(learning_rate )\n","    traing_op = optimizer.minimize(loss)\n","\n","with tf.name_scope('eval'):\n","    correct = tf.nn.in_top_k(logits, yt, 1)### Chooses the class with the top prediction score\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n"],"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-2d0114cd7d4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ytrue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'n_inputs' is not defined"]}]},{"metadata":{"id":"9hpk8gG1dvhg","colab_type":"code","colab":{}},"cell_type":"code","source":["###clip_all_weights = tf.get_collection('max_norm')\n","epochs = 80\n","batch_size = 50\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(epochs):\n","        train_gen = generator(xtrain, ytrain, batch_size )\n","        for i in range(len(xtrain)//batch_size):\n","            x_batch, y_batch = next(train_gen)\n","            sess.run(traing_op, feed_dict = {is_training: True, x: x_batch, yt: y_batch})\n","            ###sess.run(clip_all_weights)\n","        acc_train = accuracy.eval(feed_dict = {is_training: False, x: xtrain, yt: ytrain })\n","        acc_test = accuracy.eval(feed_dict = {is_training: False, x: xtest, yt: ytest })\n","        acc_dif = acc_train - acc_test\n","        \n","        print(\"-Epoch: \",epoch, \"Training Accuracy: \", acc_train, \"Test Accuracy: \",\n","              acc_test, \"Difference: \", acc_dif)\n","        \n","        save_path = saver.save(sess, \"Models/my_model.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dpJR5BzRdvhx","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.Session() as sess:\n","    saver.restore(sess, 'Models/my_model.ckpt')\n","    z = logits.eval(feed_dict ={x: load_prep(path ='test.csv', testortrain = 'test'),\n","                                is_training: False})\n","    pred = np.argmax(z, axis = 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iiDWKgoidvh7","colab_type":"code","colab":{}},"cell_type":"code","source":["predictions = pd.DataFrame(pred, index = np.arange(1, len(pred)+1), columns = ['Label'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x0a5ioFidviD","colab_type":"code","colab":{}},"cell_type":"code","source":["predictions.to_csv('predictions/nn_pred.csv', index_label = 'ImageId')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f7N3WhM0dviL","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}